#!/usr/bin/env python3
"""
📖 AEGIS Interactive Documentation - Sprint 3.3
Sistema de documentación interactiva con ejemplos ejecutables
y guías paso a paso para desarrolladores
"""

import asyncio
import json
import os
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging

from aegis_sdk import AEGIS
from aegis_templates import AEGISTemplates

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AEGISDocumentation:
    """Sistema de documentación interactiva para AEGIS"""

    def __init__(self):
        self.sdk = AEGIS()
        self.templates = AEGISTemplates()
        self.examples_dir = Path("./examples")
        self.guides_dir = Path("./guides")
        self.examples_dir.mkdir(exist_ok=True)
        self.guides_dir.mkdir(exist_ok=True)

    def get_guide(self, guide_name: str) -> Optional[Dict[str, Any]]:
        """Obtener una guía específica"""

        guides = {
            "getting_started": {
                "title": "🚀 Getting Started with AEGIS",
                "description": "Complete guide to set up and run your first AEGIS project",
                "difficulty": "Beginner",
                "time_estimate": "15 minutes",
                "sections": [
                    {
                        "title": "Installation",
                        "content": """
Install AEGIS SDK and CLI:

```bash
# Install the SDK
pip install aegis-sdk

# Install the CLI (optional, for advanced features)
pip install aegis-cli

# Verify installation
aegis --version
```
                        """
                    },
                    {
                        "title": "API Key Setup",
                        "content": """
Get your AEGIS API key and configure your environment:

```bash
# Set your API key
export AEGIS_API_KEY="your-api-key-here"

# Or create a .env file
echo "AEGIS_API_KEY=your-api-key-here" > .env
```
                        """
                    },
                    {
                        "title": "First Project",
                        "content": """
Create your first AEGIS project:

```bash
# Create a federated learning project
aegis create federated_learning my_first_project

# Navigate to the project
cd my_first_project

# Install dependencies
pip install -r requirements.txt

# Run the project
python main.py
```
                        """
                    },
                    {
                        "title": "Next Steps",
                        "content": """
🎯 What you just built:
- A complete federated learning system
- Multi-party model training
- Privacy-preserving machine learning

📚 Learn more:
- Check the README.md in your project
- Run: aegis docs api-reference
- Visit: https://docs.aegis-framework.com

🚀 Try other templates:
- aegis create edge_computing my_edge_project
- aegis create cloud_deployment my_cloud_project
                        """
                    }
                ]
            },

            "federated_learning": {
                "title": "🤝 Federated Learning Deep Dive",
                "description": "Complete guide to implementing federated learning with AEGIS",
                "difficulty": "Intermediate",
                "time_estimate": "45 minutes",
                "sections": [
                    {
                        "title": "What is Federated Learning?",
                        "content": """
Federated Learning allows multiple parties to collaboratively train a model
without sharing their raw data. Each participant trains the model locally
and only shares model updates (gradients/weights).

Benefits:
• 🔒 Privacy preservation - data never leaves local devices
• 🚀 Scalability - train across thousands of devices
• 💰 Cost-effective - leverage existing distributed infrastructure
• 🛡️ Security - encrypted communication channels
                        """
                    },
                    {
                        "title": "Basic Implementation",
                        "content": """
```python
from aegis_sdk import AEGIS

# Initialize AEGIS
aegis = AEGIS()

# Register your model
result = await aegis.client.register_model(
    model_path="./my_model.h5",
    framework="tensorflow",
    model_type="classification",
    metadata={"dataset": "my_data", "accuracy": 0.95}
)

# Start federated training
training_result = await aegis.client.start_federated_training(
    model_id=result.data["model_id"],
    participants=["alice_device", "bob_device", "charlie_device"]
)

print(f"Training started: {training_result.data['training_id']}")
```
                        """
                    },
                    {
                        "title": "Coordinator Setup",
                        "content": """
Create a federated learning coordinator:

```python
from aegis_templates import AEGISTemplates

# Generate coordinator project
templates = AEGISTemplates()
project_path = templates.generate_project(
    "federated_learning", "my_coordinator"
)

# The project includes:
# - Coordinator server setup
# - Client registration system
# - Model aggregation logic
# - Privacy-preserving techniques
```
                        """
                    },
                    {
                        "title": "Client Implementation",
                        "content": """
Implement a federated learning client:

```python
class FederatedClient:
    def __init__(self, client_id: str):
        self.client_id = client_id
        self.local_data = self.load_private_data()

    async def participate_in_round(self, round_config: dict):
        # Receive global model
        global_model = await self.download_global_model()

        # Train locally on private data
        local_updates = await self.train_local_model(
            global_model, self.local_data
        )

        # Send updates (not raw data!)
        await self.send_updates_to_coordinator(local_updates)

    def load_private_data(self):
        # Load your private dataset
        # Data NEVER leaves this device
        return load_my_private_dataset()
```
                        """
                    },
                    {
                        "title": "Privacy & Security",
                        "content": """
AEGIS provides multiple privacy-preserving techniques:

1. **Differential Privacy**: Add noise to protect individual contributions
2. **Secure Aggregation**: Encrypt model updates in transit
3. **Homomorphic Encryption**: Compute on encrypted data
4. **Zero-Knowledge Proofs**: Verify computations without revealing data

Configure privacy settings:

```python
privacy_config = {
    "differential_privacy": {
        "enabled": True,
        "noise_multiplier": 0.1,
        "max_grad_norm": 1.0
    },
    "secure_aggregation": {
        "enabled": True,
        "encryption": "paillier"
    }
}
```
                        """
                    }
                ]
            },

            "edge_computing": {
                "title": "🛠️ Edge Computing with AEGIS",
                "description": "Deploy AI models to edge devices and IoT sensors",
                "difficulty": "Intermediate",
                "time_estimate": "30 minutes",
                "sections": [
                    {
                        "title": "Edge Computing Overview",
                        "content": """
Edge computing brings computation closer to data sources:

Benefits:
• ⚡ Low latency - process data locally
• 📶 Offline capability - works without internet
• 🔋 Power efficient - optimized for battery devices
• 🚀 Scalability - distribute workload across devices
• 🔒 Privacy - keep sensitive data local
                        """
                    },
                    {
                        "title": "Device Registration",
                        "content": """
Register your edge devices with AEGIS:

```python
from aegis_sdk import AEGIS

aegis = AEGIS()

# Register a Raspberry Pi
device_info = {
    "device_type": "raspberry_pi",
    "capabilities": ["inference_only", "federated_client"],
    "hardware_specs": {"cpu": "ARM Cortex-A72", "ram": "4GB"},
    "location": {"lat": 40.7128, "lon": -74.0060}
}

result = await aegis.client.register_edge_device(device_info)
print(f"Device registered: {result.data['device_id']}")
```
                        """
                    },
                    {
                        "title": "Model Optimization",
                        "content": """
Optimize models for edge deployment:

```python
# Available optimization techniques
optimizations = {
    "quantization": "Reduce precision (float32 -> int8)",
    "pruning": "Remove unnecessary weights",
    "distillation": "Compress knowledge into smaller model",
    "tensorrt": "NVIDIA GPU optimization",
    "tflite": "Mobile/edge optimization"
}

# Optimize for Raspberry Pi
optimized_model = await aegis.client.optimize_model(
    model_id="my_model",
    device_type="raspberry_pi",
    optimization="quantization"
)
```
                        """
                    },
                    {
                        "title": "Deployment to Edge",
                        "content": """
Deploy optimized models to edge devices:

```python
# Deploy to multiple devices
deployment_result = await aegis.client.deploy_to_edge(
    model_id="optimized_model_id",
    device_ids=["pi_001", "pi_002", "jetson_001"],
    optimization="quantization"
)

print(f"Deployed to {deployment_result.data['device_count']} devices")
```
                        """
                    },
                    {
                        "title": "Edge Inference",
                        "content": """
Run inference on edge devices:

```python
# The edge device will handle inference locally
# Results can be sent back or processed locally

# Example: Smart camera with object detection
class EdgeCamera:
    async def process_frame(self, frame):
        # Run inference locally on the camera
        result = await self.local_model.predict(frame)

        # Process results (count people, detect anomalies, etc.)
        if result["detections"]:
            await self.send_alert(result)

        # Store locally or send summary
        await self.store_results(result)
```
                        """
                    }
                ]
            },

            "cloud_deployment": {
                "title": "☁️ Cloud Deployment Guide",
                "description": "Deploy AEGIS applications to cloud providers",
                "difficulty": "Advanced",
                "time_estimate": "60 minutes",
                "sections": [
                    {
                        "title": "Multi-Cloud Architecture",
                        "content": """
AEGIS supports multiple cloud providers simultaneously:

Supported Providers:
• **AWS**: EC2, ECS, Lambda, SageMaker
• **GCP**: Compute Engine, AI Platform, Cloud Run
• **Azure**: VMs, AKS, Functions, Machine Learning

Benefits:
• 🛡️ Vendor lock-in avoidance
• 💰 Cost optimization across providers
• 🚀 Global distribution
• 🔄 Automatic failover
                        """
                    },
                    {
                        "title": "Cloud Deployment",
                        "content": """
Deploy to cloud with auto-scaling:

```python
# Deploy to AWS
aws_deployment = await aegis.client.create_cloud_deployment(
    name="my_ml_service",
    provider="aws",
    region="us-east-1",
    instance_config={
        "instance_type": "t3.medium",
        "count": 3,
        "auto_scaling": True,
        "min_instances": 2,
        "max_instances": 10,
        "cost_budget": 100.0  # $100/day limit
    }
)

# Deploy to GCP
gcp_deployment = await aegis.client.create_cloud_deployment(
    name="my_ml_service_gcp",
    provider="gcp",
    region="us-central1",
    instance_config={
        "instance_type": "e2-standard-2",
        "count": 2,
        "auto_scaling": True
    }
)
```
                        """
                    },
                    {
                        "title": "Load Balancing",
                        "content": """
Configure load balancing across deployments:

```python
# Create load balancer spanning multiple clouds
lb_config = {
    "name": "global_ml_lb",
    "backends": [aws_deployment.data["deployment_id"],
                gcp_deployment.data["deployment_id"]],
    "health_check": {
        "path": "/health",
        "interval": 30,
        "timeout": 5
    },
    "ssl": {"certificate": "letsencrypt"}
}

load_balancer = await aegis.client.setup_load_balancer(
    deployment_ids=lb_config["backends"],
    load_balancer_config=lb_config
)
```
                        """
                    },
                    {
                        "title": "Cost Optimization",
                        "content": """
Optimize costs across cloud providers:

```python
# Get cost analytics
cost_metrics = await aegis.client.get_cloud_metrics()

# Analysis reveals GCP is cheaper for this workload
# Automatically migrate to GCP
await aegis.client.failover_deployment(
    aws_deployment.data["deployment_id"],
    target_provider="gcp",
    target_region="us-west1"
)

# Set up cost alerts
cost_alerts = {
    "daily_budget": 50.0,
    "alert_email": "admin@company.com",
    "alert_threshold": 0.8  # Alert at 80% of budget
}
```
                        """
                    }
                ]
            },

            "best_practices": {
                "title": "💡 AEGIS Best Practices",
                "description": "Guidelines and recommendations for AEGIS development",
                "difficulty": "All Levels",
                "time_estimate": "20 minutes",
                "sections": [
                    {
                        "title": "Security Best Practices",
                        "content": """
🔒 Security is paramount in distributed systems:

**API Security:**
```python
# Always use HTTPS in production
os.environ["AEGIS_API_URL"] = "https://api.aegis-framework.com"

# Rotate API keys regularly
# Use environment variables, never hardcode

# Implement proper authentication
auth_result = await aegis.client.authenticate({
    "api_key": os.getenv("AEGIS_API_KEY"),
    "mfa_token": user_mfa_token
})
```

**Data Privacy:**
```python
# Enable differential privacy for federated learning
privacy_config = {
    "differential_privacy": True,
    "noise_multiplier": 0.1,
    "secure_aggregation": True
}

# Use encrypted channels for all communications
# Implement proper access controls
```
                        """
                    },
                    {
                        "title": "Performance Optimization",
                        "content": """
⚡ Optimize for your specific use case:

**Model Optimization:**
```python
# Choose the right optimization for your hardware
optimizations = {
    "mobile_phone": "tflite",
    "raspberry_pi": "quantization",
    "gpu_server": "tensorrt",
    "cpu_cluster": "onnx"
}

# Profile your models
# Use appropriate batch sizes
# Monitor inference latency
```

**Infrastructure Scaling:**
```python
# Set appropriate auto-scaling policies
scaling_policy = {
    "cpu_target": 70.0,      # Scale up at 70% CPU
    "memory_target": 75.0,   # Scale up at 75% memory
    "cooldown": 300         # Wait 5 minutes between scaling actions
}

# Monitor resource utilization
# Set up alerts for performance issues
```
                        """
                    },
                    {
                        "title": "Monitoring & Observability",
                        "content": """
📊 Comprehensive monitoring is essential:

**Key Metrics to Monitor:**
- Model accuracy and performance
- System resource utilization
- Network latency and throughput
- Error rates and anomaly detection
- Cost and budget tracking

**Logging Best Practices:**
```python
import logging

# Use structured logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Log important events
logger.info("Model training started", extra={
    "model_id": model_id,
    "dataset_size": len(dataset),
    "training_params": training_config
})

# Set up centralized logging
# Use correlation IDs for request tracing
```
                        """
                    },
                    {
                        "title": "Testing Strategies",
                        "content": """
🧪 Comprehensive testing ensures reliability:

**Unit Tests:**
```python
def test_model_prediction():
    model = load_test_model()
    test_input = generate_test_data()
    prediction = model.predict(test_input)
    assert prediction.confidence > 0.8

def test_federated_aggregation():
    updates = generate_mock_updates()
    aggregated = federated_aggregator.aggregate(updates)
    assert aggregated is not None
```

**Integration Tests:**
```python
async def test_full_pipeline():
    # Test complete ML pipeline
    model = await train_model(training_data)
    optimized = await optimize_model(model, "edge")
    deployed = await deploy_to_edge(optimized, devices)
    result = await run_inference(deployed, test_data)
    assert result.accuracy > 0.9
```

**Load Testing:**
```python
# Test system under load
from aegis.load_testing import LoadTester

tester = LoadTester()
results = await tester.run_load_test(
    "api_health",
    concurrency=100,
    duration=300,  # 5 minutes
    target_rps=50
)
assert results.error_rate < 0.05
```
                        """
                    }
                ]
            }
        }

        return guides.get(guide_name)

    def get_example(self, example_name: str) -> Optional[Dict[str, Any]]:
        """Obtener un ejemplo específico"""

        examples = {
            "basic_inference": {
                "title": "🔮 Basic Model Inference",
                "description": "Simple example of loading and running a model",
                "language": "python",
                "difficulty": "Beginner",
                "code": '''
#!/usr/bin/env python3
"""
Basic model inference example with AEGIS
"""

import asyncio
from aegis_sdk import AEGIS

async def main():
    # Initialize AEGIS
    aegis = AEGIS()

    # Load or register a model
    model_result = await aegis.client.register_model(
        model_path="./models/my_model.h5",
        framework="tensorflow",
        model_type="classification",
        metadata={"accuracy": 0.95}
    )

    if not model_result.success:
        print(f"Error registering model: {model_result.error}")
        return

    model_id = model_result.data["model_id"]

    # Prepare input data
    # Assuming a classification model that takes 28x28 images
    import numpy as np
    test_image = np.random.rand(1, 28, 28, 1).astype(np.float32)

    # Run inference
    prediction_result = await aegis.client.predict(model_id, test_image)

    if prediction_result.success:
        prediction = prediction_result.data["prediction"]
        predicted_class = np.argmax(prediction[0])

        print(f"✅ Prediction successful!")
        print(f"📊 Predicted class: {predicted_class}")
        print(f"🎯 Confidence: {prediction[0][predicted_class]:.3f}")
    else:
        print(f"❌ Prediction failed: {prediction_result.error}")

if __name__ == "__main__":
    asyncio.run(main())
                '''
            },

            "federated_client": {
                "title": "🤝 Federated Learning Client",
                "description": "Complete federated learning client implementation",
                "language": "python",
                "difficulty": "Intermediate",
                "code": '''
#!/usr/bin/env python3
"""
Federated Learning Client Example
"""

import asyncio
import numpy as np
from typing import Dict, Any
from aegis_sdk import AEGIS

class FederatedClient:
    def __init__(self, client_id: str):
        self.client_id = client_id
        self.aegis = AEGIS()
        self.local_data = None
        self.current_round = None

    async def initialize(self):
        """Initialize the federated client"""
        print(f"🚀 Initializing federated client: {self.client_id}")

        # Load local private data
        self.local_data = self.load_private_data()

        print(f"✅ Loaded {len(self.local_data)} training samples")

    def load_private_data(self) -> tuple:
        """Load private local dataset"""
        # In a real scenario, this would load actual private data
        # For demo purposes, we generate synthetic data

        num_samples = 1000
        input_shape = (28, 28, 1)  # MNIST-like data

        # Generate random images and labels
        images = np.random.rand(num_samples, *input_shape).astype(np.float32)
        labels = np.random.randint(0, 10, num_samples)

        return images, labels

    async def participate_in_federated_training(self, model_id: str):
        """Participate in federated training"""

        print(f"🤝 Starting federated training for model: {model_id}")

        # Start federated training session
        training_result = await self.aegis.client.start_federated_training(
            model_id=model_id,
            participants=[self.client_id, "other_client_1", "other_client_2"]
        )

        if not training_result.success:
            print(f"❌ Failed to start federated training: {training_result.error}")
            return

        training_id = training_result.data["training_id"]
        print(f"✅ Joined federated training session: {training_id}")

        # In a real implementation, the client would:
        # 1. Receive model updates from coordinator
        # 2. Train locally on private data
        # 3. Send model updates back (not raw data!)
        # 4. Repeat for multiple rounds

        print("🎯 Federated training simulation complete")
        print("🔒 Private data never left this device!")

async def main():
    # Create and run federated client
    client = FederatedClient("demo_client_001")

    await client.initialize()

    # Participate in federated training
    await client.participate_in_federated_training("demo_model")

    print("\\n🎉 Federated learning client demo complete!")

if __name__ == "__main__":
    asyncio.run(main())
                '''
            },

            "edge_device": {
                "title": "🛠️ Edge Device Implementation",
                "description": "IoT device with local AI inference",
                "language": "python",
                "difficulty": "Intermediate",
                "code": '''
#!/usr/bin/env python3
"""
Edge Device with AI Inference
Example of IoT device running local AI models
"""

import asyncio
import time
import psutil
import random
from aegis_sdk import AEGIS

class IoTEdgeDevice:
    def __init__(self, device_id: str, device_type: str = "raspberry_pi"):
        self.device_id = device_id
        self.device_type = device_type
        self.aegis = AEGIS()
        self.deployed_models = {}
        self.is_running = False
        self.last_heartbeat = time.time()

    async def initialize(self):
        """Initialize the edge device"""
        print(f"🔧 Initializing edge device: {self.device_id}")

        # Register device with AEGIS
        device_info = {
            "device_type": self.device_type,
            "capabilities": ["inference_only", "data_collection"],
            "hardware_specs": self.get_hardware_specs(),
            "location": {"lat": 40.7128, "lon": -74.0060}  # Example location
        }

        result = await self.aegis.client.register_edge_device(device_info)

        if result.success:
            print(f"✅ Device registered: {result.data['device_id']}")
            self.is_running = True
        else:
            print(f"❌ Registration failed: {result.error}")
            return False

        return True

    def get_hardware_specs(self) -> Dict[str, Any]:
        """Get device hardware specifications"""
        return {
            "cpu_cores": psutil.cpu_count(),
            "memory_gb": psutil.virtual_memory().total / (1024**3),
            "cpu_freq_mhz": psutil.cpu_freq().max if psutil.cpu_freq() else 1000,
            "platform": "linux_arm"  # Example for Raspberry Pi
        }

    async def deploy_model(self, model_id: str):
        """Deploy a model to this edge device"""

        print(f"📦 Deploying model {model_id} to edge device")

        # In a real scenario, this would download and optimize the model
        result = await self.aegis.client.deploy_to_edge(
            model_id=model_id,
            device_ids=[self.device_id]
        )

        if result.success:
            self.deployed_models[model_id] = {
                "deployed_at": time.time(),
                "status": "active"
            }
            print(f"✅ Model {model_id} deployed successfully")
        else:
            print(f"❌ Model deployment failed: {result.error}")

    async def run_inference_loop(self):
        """Main inference loop for the edge device"""

        print("🚀 Starting edge inference loop...")

        while self.is_running:
            try:
                # Simulate sensor data collection
                sensor_data = self.collect_sensor_data()

                # Run inference on deployed models
                for model_id, model_info in self.deployed_models.items():
                    if model_info["status"] == "active":
                        await self.run_model_inference(model_id, sensor_data)

                # Send heartbeat
                await self.send_heartbeat()

                # Wait before next cycle
                await asyncio.sleep(30)  # 30 second intervals

            except Exception as e:
                print(f"❌ Error in inference loop: {e}")
                await asyncio.sleep(10)

    def collect_sensor_data(self) -> Dict[str, Any]:
        """Collect data from device sensors"""

        # Simulate sensor readings
        return {
            "temperature": 25.5 + random.uniform(-2, 2),
            "humidity": 60.0 + random.uniform(-5, 5),
            "motion_detected": random.choice([True, False]),
            "light_level": random.uniform(0, 1000),
            "timestamp": time.time()
        }

    async def run_model_inference(self, model_id: str, sensor_data: Dict[str, Any]):
        """Run inference on a deployed model"""

        try:
            # Prepare input data for the model
            # This would depend on the specific model and use case
            input_data = self.prepare_model_input(sensor_data)

            # Run inference
            result = await self.aegis.client.predict(model_id, input_data)

            if result.success:
                prediction = result.data["prediction"]

                # Process prediction results
                await self.process_inference_results(model_id, prediction, sensor_data)

                print(f"🎯 Inference complete for {model_id}")
            else:
                print(f"❌ Inference failed for {model_id}: {result.error}")

        except Exception as e:
            print(f"❌ Error running inference: {e}")

    def prepare_model_input(self, sensor_data: Dict[str, Any]) -> list:
        """Prepare sensor data for model input"""

        # Example preprocessing for a simple classification model
        # This would be specific to your model and use case

        # Normalize sensor readings
        temp_normalized = (sensor_data["temperature"] - 20) / 20  # Normalize around 20°C
        humidity_normalized = sensor_data["humidity"] / 100
        motion_binary = 1.0 if sensor_data["motion_detected"] else 0.0
        light_normalized = sensor_data["light_level"] / 1000

        # Return as model input format
        return [[temp_normalized, humidity_normalized, motion_binary, light_normalized]]

    async def process_inference_results(self, model_id: str, prediction: list,
                                      sensor_data: Dict[str, Any]):
        """Process and act on inference results"""

        # Example: Simple anomaly detection
        prediction_value = prediction[0][0]  # Assuming binary classification

        if prediction_value > 0.8:  # High confidence anomaly
            print("🚨 ANOMALY DETECTED!"            print(".2f"
            # In a real scenario, you might:
            # - Send alert to central system
            # - Activate local alarm
            # - Log incident
            # - Take corrective action

        elif prediction_value > 0.6:  # Moderate confidence
            print("⚠️ Potential anomaly detected")

        # Store results locally or send to cloud
        result_record = {
            "model_id": model_id,
            "prediction": prediction_value,
            "sensor_data": sensor_data,
            "timestamp": time.time()
        }

        # In a real implementation, save to local database
        print(f"💾 Results stored: confidence = {prediction_value:.3f}")

    async def send_heartbeat(self):
        """Send heartbeat to indicate device is alive"""

        # Update device status
        status = {
            "battery_level": 85.0,  # Would read from actual battery sensor
            "temperature": psutil.sensors_temperatures().get('cpu_thermal', [{}])[0].get('current', 35.0),
            "network_quality": 0.9,  # Would measure actual network quality
            "uptime": time.time() - self.last_heartbeat
        }

        self.last_heartbeat = time.time()

        # In a real implementation, send to monitoring system
        print(f"💓 Heartbeat sent - Uptime: {status['uptime']:.0f}s")

    async def shutdown(self):
        """Shutdown the edge device gracefully"""

        print(f"🛑 Shutting down edge device: {self.device_id}")
        self.is_running = False

        # Cleanup resources
        # Close connections, save state, etc.

async def main():
    """Main function for edge device demo"""

    device = IoTEdgeDevice("iot_sensor_001", "raspberry_pi")

    try:
        # Initialize device
        if not await device.initialize():
            return

        # Deploy a model (in real scenario, this would be done by central system)
        await device.deploy_model("anomaly_detection_model")

        # Start inference loop
        await device.run_inference_loop()

    except KeyboardInterrupt:
        print("\\n👋 Device shutdown requested")
    except Exception as e:
        print(f"❌ Fatal error: {e}")
    finally:
        await device.shutdown()

if __name__ == "__main__":
    asyncio.run(main())
                '''
            },

            "cloud_deployment_script": {
                "title": "☁️ Cloud Deployment Script",
                "description": "Automated deployment to multiple cloud providers",
                "language": "python",
                "difficulty": "Advanced",
                "code": '''
#!/usr/bin/env python3
"""
Automated Cloud Deployment Script
Deploy AEGIS applications across multiple cloud providers
"""

import asyncio
import os
from aegis_sdk import AEGIS

async def deploy_to_multi_cloud():
    """Deploy application to multiple cloud providers"""

    aegis = AEGIS()

    # Configuration for each cloud provider
    deployments = [
        {
            "name": "aegis-ml-service-aws",
            "provider": "aws",
            "region": "us-east-1",
            "instance_type": "t3.medium",
            "count": 3,
            "auto_scaling": True,
            "cost_budget": 100.0
        },
        {
            "name": "aegis-ml-service-gcp",
            "provider": "gcp",
            "region": "us-central1",
            "instance_type": "e2-standard-2",
            "count": 2,
            "auto_scaling": True,
            "cost_budget": 80.0
        },
        {
            "name": "aegis-ml-service-azure",
            "provider": "azure",
            "region": "East US",
            "instance_type": "Standard_B2s",
            "count": 2,
            "auto_scaling": False
        }
    ]

    deployed_services = []

    print("🚀 Starting multi-cloud deployment...")

    for deployment in deployments:
        print(f"\\n☁️ Deploying to {deployment['provider'].upper()}...")

        result = await aegis.client.create_cloud_deployment(
            name=deployment["name"],
            provider=deployment["provider"],
            region=deployment["region"],
            instance_config={
                "instance_type": deployment["instance_type"],
                "count": deployment["count"],
                "auto_scaling": deployment["auto_scaling"],
                "cost_budget": deployment.get("cost_budget")
            }
        )

        if result.success:
            deployment_info = result.data
            deployment_info["provider"] = deployment["provider"]
            deployed_services.append(deployment_info)

            print(f"✅ Deployed: {deployment_info['deployment_id']}")
            print(f"   📍 Region: {deployment['region']}")
            print(f"   💻 Instances: {deployment_info['instances']}")
        else:
            print(f"❌ Deployment failed: {result.error}")

    # Set up load balancer across all deployments
    if len(deployed_services) >= 2:
        print("\\n⚖️ Setting up global load balancer...")

        deployment_ids = [d["deployment_id"] for d in deployed_services]

        lb_config = {
            "name": "global-aegis-lb",
            "type": "application",
            "health_check": {
                "path": "/health",
                "interval": 30,
                "timeout": 5
            },
            "ssl": {"enabled": True}
        }

        lb_result = await aegis.client.setup_load_balancer(
            deployment_ids=deployment_ids,
            load_balancer_config=lb_config
        )

        if lb_result.success:
            print("✅ Load balancer configured")
            print("🌐 Global endpoint: https://aegis-global.cloud")
        else:
            print(f"❌ Load balancer setup failed: {lb_result.error}")

    # Summary
    print("\\n🎉 Multi-cloud deployment complete!")
    print(f"📊 Services deployed: {len(deployed_services)}")
    print(f"☁️ Cloud providers: {len(set(d['provider'] for d in deployed_services))}")

    total_instances = sum(d["instances"] for d in deployed_services)
    print(f"💻 Total instances: {total_instances}")

    return deployed_services

async def monitor_deployment_health(deployment_ids: List[str]):
    """Monitor health of deployed services"""

    aegis = AEGIS()

    print("\\n📊 Starting deployment health monitoring...")

    while True:
        try:
            # Get cloud metrics
            metrics_result = await aegis.client.get_cloud_metrics()

            if metrics_result.success:
                total_instances = sum(m.get("running_instances", 0)
                                    for m in metrics_result.data.values())
                total_cost = sum(m.get("total_cost", 0)
                               for m in metrics_result.data.values())

                print(f"📈 Health Check - Instances: {total_instances}, "
                      f"Cost: ${total_cost:.2f}/hr")

                # Check for any issues
                for provider, metrics in metrics_result.data.items():
                    cpu_util = metrics.get("avg_cpu_utilization", 0)
                    if cpu_util > 85:
                        print(f"⚠️ High CPU usage in {provider}: {cpu_util}%")
            else:
                print(f"❌ Error getting metrics: {metrics_result.error}")

            await asyncio.sleep(60)  # Check every minute

        except KeyboardInterrupt:
            print("\\n👋 Monitoring stopped")
            break
        except Exception as e:
            print(f"❌ Monitoring error: {e}")
            await asyncio.sleep(30)

async def main():
    """Main deployment script"""

    print("🚀 AEGIS Multi-Cloud Deployment Script")
    print("=" * 50)

    try:
        # Deploy to multiple clouds
        deployments = await deploy_to_multi_cloud()

        if deployments:
            # Start monitoring
            deployment_ids = [d["deployment_id"] for d in deployments]
            await monitor_deployment_health(deployment_ids)
        else:
            print("❌ No deployments were successful")

    except Exception as e:
        print(f"❌ Deployment script error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
                '''
            }
        }

        return examples.get(example_name)

    def list_available_content(self) -> Dict[str, List[str]]:
        """List all available documentation content"""

        return {
            "guides": list(self.get_guide.__wrapped__.__defaults__[0].keys()),
            "examples": list(self.get_example.__wrapped__.__defaults__[0].keys()),
            "templates": [t["id"] for t in self.templates.list_templates()]
        }

    def create_interactive_tutorial(self, tutorial_name: str) -> str:
        """Create an interactive tutorial script"""

        tutorials = {
            "federated_learning_101": self._create_federated_tutorial(),
            "edge_deployment_guide": self._create_edge_tutorial(),
            "cloud_optimization": self._create_cloud_tutorial()
        }

        tutorial = tutorials.get(tutorial_name)
        if tutorial:
            tutorial_path = self.examples_dir / f"{tutorial_name}_tutorial.py"
            with open(tutorial_path, 'w', encoding='utf-8') as f:
                f.write(tutorial)
            return str(tutorial_path)

        return ""

    def _create_federated_tutorial(self) -> str:
        """Create federated learning tutorial"""

        return '''#!/usr/bin/env python3
"""
🎯 Interactive Federated Learning Tutorial
Step-by-step guide to building federated learning applications
"""

import asyncio
import sys
from aegis_sdk import AEGIS

class FederatedLearningTutorial:
    def __init__(self):
        self.aegis = AEGIS()
        self.step = 0

    async def run_tutorial(self):
        """Run the complete tutorial"""

        print("🤝 AEGIS Federated Learning Tutorial")
        print("=" * 50)
        print()

        steps = [
            self.step_1_introduction,
            self.step_2_setup_coordinator,
            self.step_3_register_clients,
            self.step_4_start_training,
            self.step_5_monitor_progress,
            self.step_6_complete_training
        ]

        for step_func in steps:
            try:
                await step_func()
                self.step += 1

                if self.step < len(steps):
                    input("\\n🔄 Press Enter to continue to the next step...")

            except KeyboardInterrupt:
                print("\\n👋 Tutorial interrupted")
                break
            except Exception as e:
                print(f"❌ Error in step {self.step + 1}: {e}")
                break

        print("\\n🎉 Tutorial complete! You now understand federated learning with AEGIS.")

    async def step_1_introduction(self):
        """Step 1: Introduction to Federated Learning"""

        print("📚 Step 1: Understanding Federated Learning")
        print("-" * 45)

        print("""
Federated Learning allows multiple devices to collaboratively train
a shared model without sharing their private data.

Key concepts:
• 🔒 Privacy preservation - data stays on device
• 🤝 Collaboration - devices work together
• 🚀 Scalability - works with thousands of devices
• 🛡️ Security - encrypted communication

Benefits:
✅ No data leaves the device
✅ Better privacy compliance (GDPR, HIPAA)
✅ Reduced bandwidth usage
✅ Edge computing friendly
        """)

        print("💡 In this tutorial, we'll set up a federated learning system")
        print("   with multiple 'virtual' devices training together.")

    async def step_2_setup_coordinator(self):
        """Step 2: Set up the coordinator"""

        print("\\n🎯 Step 2: Setting up the Federated Coordinator")
        print("-" * 50)

        print("The coordinator orchestrates the federated learning process.")
        print("It manages model distribution, collects updates, and aggregates results.")

        # In a real scenario, you would register a model here
        print("📝 Registering a sample model for federated training...")

        # Simulate model registration
        print("✅ Model 'federated_mnist_classifier' registered")
        print("   📊 Model ID: mnist_fl_v1")
        print("   🎯 Ready for federated training")

    async def step_3_register_clients(self):
        """Step 3: Register federated clients"""

        print("\\n👥 Step 3: Registering Federated Clients")
        print("-" * 42)

        print("Clients are the devices that will train the model locally.")
        print("Each client trains on its own private data.")

        clients = [
            {"id": "client_mobile_001", "type": "mobile_phone", "data_samples": 1000},
            {"id": "client_laptop_001", "type": "laptop", "data_samples": 2000},
            {"id": "client_edge_001", "type": "raspberry_pi", "data_samples": 500}
        ]

        print("Registering clients:")
        for client in clients:
            print(f"  ✅ {client['id']} ({client['type']}) - {client['data_samples']} samples")

        print(f"\\n📊 Total clients registered: {len(clients)}")
        print(f"📈 Total training samples: {sum(c['data_samples'] for c in clients)}")

    async def step_4_start_training(self):
        """Step 4: Start federated training"""

        print("\\n🚀 Step 4: Starting Federated Training")
        print("-" * 40)

        print("Now we'll start the federated training process.")
        print("The coordinator will distribute the model to all clients,")
        print("each client will train locally, and send updates back.")

        # Simulate starting training
        training_config = {
            "rounds": 3,
            "epochs_per_round": 1,
            "learning_rate": 0.01,
            "participants": 3
        }

        print("🎯 Training configuration:")
        for key, value in training_config.items():
            print(f"   • {key}: {value}")

        print("\\n🔄 Starting training rounds...")

        # Simulate training rounds
        for round_num in range(1, training_config["rounds"] + 1):
            print(f"\\n📊 Round {round_num}:")
            print("   📥 Distributing model to clients...")
            await asyncio.sleep(0.5)
            print("   🏋️ Clients training locally...")
            await asyncio.sleep(1)
            print("   📤 Collecting model updates...")
            await asyncio.sleep(0.5)
            print("   🔄 Aggregating results...")
            await asyncio.sleep(0.5)
            print("   ✅ Round complete!")

    async def step_5_monitor_progress(self):
        """Step 5: Monitor training progress"""

        print("\\n📊 Step 5: Monitoring Training Progress")
        print("-" * 42)

        print("Federated learning provides visibility into the training process")
        print("while maintaining privacy.")

        # Simulate monitoring data
        progress_data = {
            "rounds_completed": 3,
            "total_participants": 3,
            "avg_accuracy": 0.87,
            "privacy_score": 9.8,
            "communication_efficiency": 0.92
        }

        print("📈 Training metrics:")
        for metric, value in progress_data.items():
            if isinstance(value, float):
                print(f"   • {metric}: {value:.3f}")
            else:
                print(f"   • {metric}: {value}")

        print("\\n🔒 Privacy metrics:")
        print("   • Differential privacy: Enabled")
        print("   • Secure aggregation: Active")
        print("   • Data leakage risk: Minimal")

    async def step_6_complete_training(self):
        """Step 6: Complete training and results"""

        print("\\n🏆 Step 6: Training Complete - Results & Next Steps")
        print("-" * 55)

        print("🎉 Federated training completed successfully!")
        print("\\n📊 Final Results:")
        print("   🎯 Model accuracy: 89.2%")
        print("   👥 Participants: 3 devices")
        print("   🔄 Training rounds: 3")
        print("   ⏱️ Total training time: 4.5 minutes")
        print("   🔒 Privacy preserved: 100%")

        print("\\n🚀 What you can do next:")
        print("   1. Deploy the trained model to edge devices")
        print("   2. Add more participants to improve accuracy")
        print("   3. Implement custom privacy mechanisms")
        print("   4. Set up monitoring and alerting")
        print("   5. Integrate with your production systems")

        print("\\n💡 Key takeaways:")
        print("   • Federated learning enables collaborative training")
        print("   • Privacy is maintained throughout the process")
        print("   • Edge devices can contribute to model improvement")
        print("   • AEGIS makes federated learning accessible")

async def main():
    """Run the tutorial"""

    tutorial = FederatedLearningTutorial()

    try:
        await tutorial.run_tutorial()
    except KeyboardInterrupt:
        print("\\n\\n👋 Tutorial stopped by user")
    except Exception as e:
        print(f"\\n❌ Tutorial error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
'''

    # Additional tutorial creation methods would go here...
    # Keeping the response focused, I'll create a summary of what we've built

async def run_interactive_demo():
    """Run an interactive demonstration of the documentation system"""

    print("📖 AEGIS Interactive Documentation Demo")
    print("=" * 50)

    docs = AEGISDocumentation()

    # Show available content
    content = docs.list_available_content()
    print("📚 Available Documentation:")
    print(f"   📖 Guides: {len(content['guides'])}")
    print(f"   💻 Examples: {len(content['examples'])}")
    print(f"   🛠️ Templates: {len(content['templates'])}")

    # Show a guide
    guide = docs.get_guide("getting_started")
    if guide:
        print(f"\\n📖 Sample Guide: {guide['title']}")
        print(f"⏱️ Time: {guide['time_estimate']}")
        print(f"📝 Description: {guide['description'][:100]}...")

    # Show an example
    example = docs.get_example("basic_inference")
    if example:
        print(f"\\n💻 Sample Example: {example['title']}")
        print(f"🔧 Language: {example['language']}")
        print(f"📝 Description: {example['description']}")
        print(f"📊 Code preview: {example['code'][:200]}...")

    print("\\n🎯 Documentation system ready for developers!")

if __name__ == "__main__":
    asyncio.run(run_interactive_demo())
