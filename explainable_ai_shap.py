#!/usr/bin/env python3
"""
üß† AEGIS Explainable AI with SHAP - Sprint 4.2
Sistema completo de IA explicable con integraci√≥n de SHAP y t√©cnicas avanzadas
"""

import asyncio
import time
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple, Callable, Union
from dataclasses import dataclass, field
from enum import Enum
import logging
import warnings
import matplotlib.pyplot as plt
import seaborn as sns

# Intentar importar SHAP
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    warnings.warn("SHAP no est√° disponible. Instalar con: pip install shap")

# Importar componentes del framework
from ml_framework_integration import MLFrameworkManager, MLFramework

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ExplanationMethod(Enum):
    """M√©todos de explicaci√≥n disponibles"""
    SHAP_VALUES = "shap_values"
    LIME = "lime"
    PERMUTATION_IMPORTANCE = "permutation_importance"
    PARTIAL_DEPENDENCE = "partial_dependence"
    FEATURE_INTERACTIONS = "feature_interactions"
    GRADIENT_BASED = "gradient_based"
    DEEP_TAYLOR = "deep_taylor"

class ExplanationScope(Enum):
    """Alcance de la explicaci√≥n"""
    LOCAL = "local"      # Explicaci√≥n de una predicci√≥n individual
    GLOBAL = "global"    # Explicaci√≥n del modelo completo
    COHORT = "cohort"    # Explicaci√≥n de un grupo de predicciones

@dataclass
class SHAPExplanation:
    """Explicaci√≥n usando SHAP"""
    method: ExplanationMethod
    scope: ExplanationScope
    shap_values: Optional[np.ndarray] = None
    base_values: Optional[np.ndarray] = None
    data: Optional[np.ndarray] = None
    feature_names: Optional[List[str]] = None
    explanation_text: str = ""
    visualization_data: Dict[str, Any] = field(default_factory=dict)
    processing_time: float = 0.0

@dataclass
class FeatureImportance:
    """Importancia de features"""
    feature_name: str
    importance_score: float
    std_dev: Optional[float] = None
    rank: Optional[int] = None
    contribution_direction: str = "positive"  # positive, negative, mixed

@dataclass
class ModelExplanation:
    """Explicaci√≥n completa del modelo"""
    model_name: str
    model_type: str
    global_explanations: List[FeatureImportance] = field(default_factory=list)
    local_explanations: Dict[str, SHAPExplanation] = field(default_factory=dict)
    interaction_effects: Dict[Tuple[str, str], float] = field(default_factory=dict)
    model_insights: List[str] = field(default_factory=list)
    fairness_analysis: Dict[str, Any] = field(default_factory=dict)
    robustness_analysis: Dict[str, Any] = field(default_factory=dict)

# ===== EXPLICADORES DE SHAP =====

class SHAPExplainer:
    """Explainer basado en SHAP"""

    def __init__(self):
        if not SHAP_AVAILABLE:
            raise ImportError("SHAP no est√° disponible. Instalar con: pip install shap")

        self.explainer_cache: Dict[str, Any] = {}

    def create_explainer(self, model: Any, background_data: np.ndarray,
                        model_type: str = "auto") -> Any:
        """Crear explainer SHAP apropiado para el modelo"""

        cache_key = f"{model_type}_{hash(background_data.tobytes()):x}"

        if cache_key in self.explainer_cache:
            return self.explainer_cache[cache_key]

        try:
            if model_type == "tree" or hasattr(model, 'predict_proba'):
                # Para modelos de √°rbol o sklearn
                explainer = shap.TreeExplainer(model, background_data)
            elif model_type == "linear":
                explainer = shap.LinearExplainer(model, background_data)
            elif model_type == "deep" or isinstance(model, nn.Module):
                # Para modelos de deep learning
                explainer = shap.DeepExplainer(model, background_data)
            else:
                # Fallback: KernelExplainer
                explainer = shap.KernelExplainer(model.predict, background_data)

            self.explainer_cache[cache_key] = explainer
            return explainer

        except Exception as e:
            logger.warning(f"Error creando SHAP explainer: {e}")
            # Fallback simplificado
            return None

    def explain_prediction(self, explainer: Any, input_data: np.ndarray,
                          max_evals: int = 100) -> SHAPExplanation:
        """Explicar una predicci√≥n individual"""

        start_time = time.time()

        try:
            # Calcular SHAP values
            shap_values = explainer.shap_values(input_data, max_evals=max_evals)

            # Para clasificaci√≥n m√∫ltiple, tomar la clase positiva
            if isinstance(shap_values, list) and len(shap_values) > 1:
                # Tomar la clase con mayor valor absoluto
                shap_values_combined = np.sum(np.abs(shap_values), axis=0)
                shap_values = shap_values[np.argmax(np.abs(shap_values).sum(axis=(1, 2)))]
            elif isinstance(shap_values, list):
                shap_values = shap_values[0]

            processing_time = time.time() - start_time

            return SHAPExplanation(
                method=ExplanationMethod.SHAP_VALUES,
                scope=ExplanationScope.LOCAL,
                shap_values=shap_values,
                data=input_data,
                processing_time=processing_time,
                explanation_text=self._generate_local_explanation_text(shap_values, input_data)
            )

        except Exception as e:
            logger.error(f"Error explicando predicci√≥n: {e}")
            return SHAPExplanation(
                method=ExplanationMethod.SHAP_VALUES,
                scope=ExplanationScope.LOCAL,
                processing_time=time.time() - start_time
            )

    def explain_model_global(self, explainer: Any, background_data: np.ndarray,
                           feature_names: Optional[List[str]] = None) -> List[FeatureImportance]:
        """Explicaci√≥n global del modelo"""

        try:
            # Calcular SHAP values para m√∫ltiples samples
            sample_size = min(100, len(background_data))
            sample_indices = np.random.choice(len(background_data), sample_size, replace=False)
            sample_data = background_data[sample_indices]

            shap_values = explainer.shap_values(sample_data)

            # Procesar seg√∫n el tipo de modelo
            if isinstance(shap_values, list):
                # Multi-class classification
                shap_values = shap_values[0]  # Tomar primera clase

            # Calcular importancia global
            feature_importance = np.abs(shap_values).mean(axis=0)

            # Crear objetos FeatureImportance
            importance_list = []
            for i, importance in enumerate(feature_importance):
                feature_name = feature_names[i] if feature_names and i < len(feature_names) else f"feature_{i}"

                importance_list.append(FeatureImportance(
                    feature_name=feature_name,
                    importance_score=float(importance),
                    rank=i + 1
                ))

            # Ordenar por importancia
            importance_list.sort(key=lambda x: x.importance_score, reverse=True)

            return importance_list

        except Exception as e:
            logger.error(f"Error en explicaci√≥n global: {e}")
            return []

    def _generate_local_explanation_text(self, shap_values: np.ndarray,
                                       input_data: np.ndarray) -> str:
        """Generar texto explicativo para explicaci√≥n local"""

        if shap_values.ndim == 1:
            # Single output
            top_features = np.argsort(np.abs(shap_values))[-3:][::-1]  # Top 3 features

            explanation = "Esta predicci√≥n se explica principalmente por: "
            for i, feature_idx in enumerate(top_features):
                impact = shap_values[feature_idx]
                direction = "aumenta" if impact > 0 else "disminuye"
                explanation += ".2f"
                if i < len(top_features) - 1:
                    explanation += ", "

        else:
            explanation = f"Explicaci√≥n generada para {shap_values.shape[1]} features"

        return explanation

# ===== ALTERNATIVAS CUANDO SHAP NO EST√Å DISPONIBLE =====

class FallbackExplainer:
    """Explainer alternativo cuando SHAP no est√° disponible"""

    def __init__(self):
        self.feature_importance_cache: Dict[str, List[FeatureImportance]] = {}

    def explain_prediction_permutation(self, model: Any, input_data: np.ndarray,
                                     background_data: np.ndarray,
                                     feature_names: Optional[List[str]] = None) -> SHAPExplanation:
        """Explicaci√≥n usando permutation importance"""

        start_time = time.time()

        try:
            # Baseline prediction
            baseline_pred = model.predict(input_data)

            # Calcular importancia permutando cada feature
            importance_scores = []
            n_features = input_data.shape[1]

            for i in range(n_features):
                # Permutar feature i
                permuted_data = input_data.copy()
                permuted_data[0, i] = np.random.permutation(background_data[:, i])[:1]

                # Nueva predicci√≥n
                permuted_pred = model.predict(permuted_data)

                # Calcular importancia (cambio en predicci√≥n)
                importance = abs(baseline_pred[0] - permuted_pred[0])
                importance_scores.append(importance)

            # Convertir a array similar a SHAP
            shap_like_values = np.array(importance_scores)

            processing_time = time.time() - start_time

            return SHAPExplanation(
                method=ExplanationMethod.PERMUTATION_IMPORTANCE,
                scope=ExplanationScope.LOCAL,
                shap_values=shap_like_values,
                data=input_data,
                processing_time=processing_time,
                explanation_text=self._generate_permutation_explanation(shap_like_values, feature_names)
            )

        except Exception as e:
            logger.error(f"Error en permutation explanation: {e}")
            return SHAPExplanation(
                method=ExplanationMethod.PERMUTATION_IMPORTANCE,
                scope=ExplanationScope.LOCAL,
                processing_time=time.time() - start_time
            )

    def explain_model_permutation(self, model: Any, X: np.ndarray, y: np.ndarray,
                                feature_names: Optional[List[str]] = None,
                                n_repeats: int = 5) -> List[FeatureImportance]:
        """Importancia de features usando permutation importance"""

        try:
            from sklearn.inspection import permutation_importance
            from sklearn.metrics import accuracy_score, r2_score

            # Calcular permutation importance
            if hasattr(model, 'predict_proba'):
                # Classification
                y_pred = model.predict(X)
                baseline_score = accuracy_score(y, y_pred)

                perm_importance = permutation_importance(
                    model, X, y, scoring='accuracy', n_repeats=n_repeats, random_state=42
                )
            else:
                # Regression
                y_pred = model.predict(X)
                baseline_score = r2_score(y, y_pred)

                perm_importance = permutation_importance(
                    model, X, y, scoring='r2', n_repeats=n_repeats, random_state=42
                )

            # Crear lista de importancia
            importance_list = []
            for i, (importance, std) in enumerate(zip(perm_importance.importances_mean,
                                                    perm_importance.importances_std)):
                feature_name = feature_names[i] if feature_names and i < len(feature_names) else f"feature_{i}"

                importance_list.append(FeatureImportance(
                    feature_name=feature_name,
                    importance_score=float(importance),
                    std_dev=float(std),
                    rank=i + 1
                ))

            # Ordenar por importancia
            importance_list.sort(key=lambda x: x.importance_score, reverse=True)

            return importance_list

        except Exception as e:
            logger.error(f"Error en permutation importance: {e}")
            return []

    def _generate_permutation_explanation(self, importance_scores: np.ndarray,
                                        feature_names: Optional[List[str]] = None) -> str:
        """Generar explicaci√≥n basada en permutation importance"""

        top_features = np.argsort(importance_scores)[-3:][::-1]  # Top 3

        explanation = "La predicci√≥n cambia m√°s cuando se modifica: "
        for i, feature_idx in enumerate(top_features):
            feature_name = feature_names[feature_idx] if feature_names and feature_idx < len(feature_names) else f"feature_{feature_idx}"
            explanation += f"{feature_name} ({importance_scores[feature_idx]:.3f})"
            if i < len(top_features) - 1:
                explanation += ", "

        return explanation

# ===== EXPLICACIONES PARA DEEP LEARNING =====

class DeepLearningExplainer:
    """Explicaciones espec√≠ficas para modelos de deep learning"""

    def __init__(self):
        self.gradients_cache: Dict[str, Any] = {}

    def explain_with_gradients(self, model: nn.Module, input_tensor: torch.Tensor,
                              target_class: Optional[int] = None) -> SHAPExplanation:
        """Explicaci√≥n usando gradients (GradCAM-like)"""

        start_time = time.time()

        try:
            model.eval()
            input_tensor.requires_grad_(True)

            # Forward pass
            output = model(input_tensor)

            if target_class is None:
                target_class = output.argmax(dim=1).item()

            # Backward pass para gradients
            model.zero_grad()
            output[0, target_class].backward()

            # Obtener gradients
            gradients = input_tensor.grad.detach().numpy()

            # Calcular importancia (magnitud del gradiente)
            importance_scores = np.abs(gradients).mean(axis=(0, 2, 3)) if gradients.ndim > 2 else np.abs(gradients).flatten()

            processing_time = time.time() - start_time

            return SHAPExplanation(
                method=ExplanationMethod.GRADIENT_BASED,
                scope=ExplanationScope.LOCAL,
                shap_values=importance_scores,
                data=input_tensor.detach().numpy(),
                processing_time=processing_time,
                explanation_text=f"Gradients explican la predicci√≥n de clase {target_class}"
            )

        except Exception as e:
            logger.error(f"Error en gradient explanation: {e}")
            return SHAPExplanation(
                method=ExplanationMethod.GRADIENT_BASED,
                scope=ExplanationScope.LOCAL,
                processing_time=time.time() - start_time
            )

    def explain_with_deep_taylor(self, model: nn.Module, input_tensor: torch.Tensor) -> SHAPExplanation:
        """Explicaci√≥n usando Deep Taylor Decomposition (simplificada)"""

        start_time = time.time()

        try:
            # Implementaci√≥n simplificada de Deep Taylor
            # En producci√≥n, usar librer√≠as especializadas como Innvestigate

            model.eval()

            # Calcular relevancia propagando hacia atr√°s
            relevance_scores = self._propagate_relevance(model, input_tensor)

            processing_time = time.time() - start_time

            return SHAPExplanation(
                method=ExplanationMethod.DEEP_TAYLOR,
                scope=ExplanationScope.LOCAL,
                shap_values=relevance_scores,
                data=input_tensor.detach().numpy(),
                processing_time=processing_time,
                explanation_text="Relevance scores calculados usando Deep Taylor Decomposition"
            )

        except Exception as e:
            logger.error(f"Error en Deep Taylor explanation: {e}")
            return SHAPExplanation(
                method=ExplanationMethod.DEEP_TAYLOR,
                scope=ExplanationScope.LOCAL,
                processing_time=time.time() - start_time
            )

    def _propagate_relevance(self, model: nn.Module, input_tensor: torch.Tensor) -> np.ndarray:
        """Propagar relevance scores (implementaci√≥n simplificada)"""

        # Esta es una implementaci√≥n muy simplificada
        # En producci√≥n, implementar LRP (Layer-wise Relevance Propagation) correctamente

        try:
            # Obtener activaciones finales
            with torch.no_grad():
                output = model(input_tensor)

            # Usar la magnitud de la salida como proxy de relevance
            relevance = output.abs().detach().numpy()

            # Flatten para obtener scores por feature
            return relevance.flatten()

        except Exception:
            # Fallback: importancia uniforme
            return np.ones(input_tensor.numel()) / input_tensor.numel()

# ===== VISUALIZACIONES =====

class ExplanationVisualizer:
    """Visualizador de explicaciones"""

    def __init__(self):
        self.plots = {}

    def plot_shap_waterfall(self, shap_explanation: SHAPExplanation,
                           feature_names: Optional[List[str]] = None,
                           title: str = "SHAP Waterfall Plot") -> plt.Figure:
        """Crear waterfall plot de SHAP values"""

        try:
            if shap_explanation.shap_values is None:
                return None

            fig, ax = plt.subplots(figsize=(10, 6))

            shap_values = shap_explanation.shap_values.flatten()
            n_features = len(shap_values)

            # Usar nombres de features o √≠ndices
            if feature_names and len(feature_names) >= n_features:
                labels = feature_names[:n_features]
            else:
                labels = [f"Feature {i}" for i in range(n_features)]

            # Ordenar por importancia
            sorted_indices = np.argsort(np.abs(shap_values))[::-1]
            sorted_values = shap_values[sorted_indices]
            sorted_labels = [labels[i] for i in sorted_indices]

            # Crear waterfall plot
            cumulative = np.cumsum(sorted_values)
            cumulative = np.insert(cumulative, 0, 0)

            # Plot
            ax.bar(range(len(sorted_values)), sorted_values, bottom=cumulative[:-1],
                  label='SHAP Values', alpha=0.7)

            ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)
            ax.set_xlabel('Features')
            ax.set_ylabel('SHAP Value')
            ax.set_title(title)
            ax.set_xticks(range(len(sorted_labels)))
            ax.set_xticklabels(sorted_labels, rotation=45, ha='right')

            plt.tight_layout()
            return fig

        except Exception as e:
            logger.error(f"Error creando waterfall plot: {e}")
            return None

    def plot_feature_importance(self, importance_list: List[FeatureImportance],
                               title: str = "Feature Importance") -> plt.Figure:
        """Plot de importancia de features"""

        try:
            fig, ax = plt.subplots(figsize=(12, 8))

            # Tomar top 20 features
            top_features = importance_list[:20]
            names = [f.feature_name for f in top_features]
            scores = [f.importance_score for f in top_features]

            # Crear horizontal bar plot
            bars = ax.barh(range(len(names)), scores, alpha=0.7)
            ax.set_yticks(range(len(names)))
            ax.set_yticklabels(names)
            ax.set_xlabel('Importance Score')
            ax.set_title(title)

            # Agregar valores en las barras
            for i, (bar, score) in enumerate(zip(bars, scores)):
                ax.text(bar.get_width() + max(scores) * 0.01, bar.get_y() + bar.get_height()/2,
                       '.3f', ha='left', va='center', fontsize=8)

            plt.tight_layout()
            return fig

        except Exception as e:
            logger.error(f"Error creando feature importance plot: {e}")
            return None

    def plot_shap_summary(self, shap_values: np.ndarray, feature_names: Optional[List[str]] = None,
                         title: str = "SHAP Summary Plot") -> plt.Figure:
        """Crear summary plot de SHAP (simplificado)"""

        try:
            fig, ax = plt.subplots(figsize=(10, 6))

            if shap_values.ndim == 1:
                # Single prediction
                shap_values = shap_values.reshape(1, -1)

            # Calcular mean absolute SHAP values
            mean_abs_shap = np.abs(shap_values).mean(axis=0)

            n_features = len(mean_abs_shap)
            if feature_names and len(feature_names) >= n_features:
                labels = feature_names[:n_features]
            else:
                labels = [f"Feature {i}" for i in range(n_features)]

            # Ordenar
            sorted_indices = np.argsort(mean_abs_shap)[::-1]
            sorted_values = mean_abs_shap[sorted_indices]
            sorted_labels = [labels[i] for i in sorted_indices]

            # Plot
            ax.bar(range(len(sorted_values)), sorted_values, alpha=0.7)
            ax.set_xlabel('Features')
            ax.set_ylabel('Mean |SHAP Value|')
            ax.set_title(title)
            ax.set_xticks(range(len(sorted_labels)))
            ax.set_xticklabels(sorted_labels, rotation=45, ha='right')

            plt.tight_layout()
            return fig

        except Exception as e:
            logger.error(f"Error creando SHAP summary plot: {e}")
            return None

# ===== SISTEMA PRINCIPAL =====

class AEGISExplainableAI:
    """Sistema completo de IA Explicable"""

    def __init__(self):
        self.shap_explainer = SHAPExplainer() if SHAP_AVAILABLE else None
        self.fallback_explainer = FallbackExplainer()
        self.dl_explainer = DeepLearningExplainer()
        self.visualizer = ExplanationVisualizer()
        self.explanations_cache: Dict[str, Any] = {}

    async def explain_model_prediction(self, model: Any, input_data: Union[np.ndarray, pd.DataFrame],
                                     background_data: Optional[np.ndarray] = None,
                                     model_type: str = "auto",
                                     feature_names: Optional[List[str]] = None) -> SHAPExplanation:
        """Explicar una predicci√≥n individual"""

        logger.info("üîç Explicando predicci√≥n del modelo...")

        start_time = time.time()

        # Preparar datos
        if isinstance(input_data, pd.DataFrame):
            input_array = input_data.values
            if feature_names is None:
                feature_names = input_data.columns.tolist()
        else:
            input_array = np.array(input_data)

        # Preparar background data
        if background_data is None:
            # Crear background sint√©tico
            background_data = np.random.normal(0, 1, (50, input_array.shape[1]))

        try:
            # Intentar SHAP primero
            if self.shap_explainer and SHAP_AVAILABLE:
                explainer = self.shap_explainer.create_explainer(model, background_data, model_type)
                if explainer:
                    explanation = self.shap_explainer.explain_prediction(explainer, input_array)
                    explanation.processing_time = time.time() - start_time
                    return explanation

            # Fallback: permutation importance
            logger.info("Usando m√©todo alternativo (permutation importance)")
            explanation = self.fallback_explainer.explain_prediction_permutation(
                model, input_array, background_data, feature_names
            )
            explanation.processing_time = time.time() - start_time
            return explanation

        except Exception as e:
            logger.error(f"Error explicando predicci√≥n: {e}")
            return SHAPExplanation(
                method=ExplanationMethod.PERMUTATION_IMPORTANCE,
                scope=ExplanationScope.LOCAL,
                processing_time=time.time() - start_time
            )

    async def explain_model_global(self, model: Any, X_train: np.ndarray, y_train: np.ndarray,
                                 model_type: str = "auto",
                                 feature_names: Optional[List[str]] = None) -> List[FeatureImportance]:
        """Explicaci√≥n global del modelo"""

        logger.info("üåç Generando explicaci√≥n global del modelo...")

        try:
            # Intentar SHAP primero
            if self.shap_explainer and SHAP_AVAILABLE:
                background_sample = X_train[np.random.choice(len(X_train), min(100, len(X_train)), replace=False)]
                explainer = self.shap_explainer.create_explainer(model, background_sample, model_type)

                if explainer:
                    importance_list = self.shap_explainer.explain_model_global(
                        explainer, background_sample, feature_names
                    )
                    if importance_list:
                        return importance_list

            # Fallback: permutation importance
            logger.info("Usando m√©todo alternativo (permutation importance)")
            importance_list = self.fallback_explainer.explain_model_permutation(
                model, X_train, y_train, feature_names
            )

            return importance_list

        except Exception as e:
            logger.error(f"Error en explicaci√≥n global: {e}")
            return []

    async def explain_deep_learning_model(self, model: nn.Module, input_tensor: torch.Tensor,
                                        method: ExplanationMethod = ExplanationMethod.GRADIENT_BASED) -> SHAPExplanation:
        """Explicar modelo de deep learning"""

        logger.info(f"üß† Explicando modelo deep learning usando {method.value}")

        if method == ExplanationMethod.GRADIENT_BASED:
            return self.dl_explainer.explain_with_gradients(model, input_tensor)
        elif method == ExplanationMethod.DEEP_TAYLOR:
            return self.dl_explainer.explain_with_deep_taylor(model, input_tensor)
        else:
            # Fallback
            return self.dl_explainer.explain_with_gradients(model, input_tensor)

    def create_model_explanation(self, model: Any, model_name: str, X_train: np.ndarray,
                               y_train: np.ndarray, model_type: str = "auto",
                               feature_names: Optional[List[str]] = None) -> ModelExplanation:
        """Crear explicaci√≥n completa del modelo"""

        logger.info(f"üìã Creando explicaci√≥n completa del modelo {model_name}")

        explanation = ModelExplanation(
            model_name=model_name,
            model_type=model_type
        )

        # Explicaci√≥n global
        asyncio.run(self.explain_model_global(model, X_train, y_train, model_type, feature_names))

        # Insights del modelo
        explanation.model_insights = self._generate_model_insights(
            explanation.global_explanations, model_type
        )

        # An√°lisis de fairness (simplificado)
        explanation.fairness_analysis = self._analyze_model_fairness(X_train, y_train)

        # An√°lisis de robustness (simplificado)
        explanation.robustness_analysis = self._analyze_model_robustness(model, X_train)

        return explanation

    def _generate_model_insights(self, global_explanations: List[FeatureImportance],
                               model_type: str) -> List[str]:
        """Generar insights autom√°ticos sobre el modelo"""

        insights = []

        if not global_explanations:
            return ["No se pudieron generar explicaciones para este modelo"]

        # Top features
        top_feature = global_explanations[0]
        insights.append(f"üîù La feature m√°s importante es '{top_feature.feature_name}' "
                       ".3f"
        # N√∫mero de features importantes
        important_features = [f for f in global_explanations if f.importance_score > 0.01]
        insights.append(f"üìä {len(important_features)} features tienen importancia significativa (>0.01)")

        # Insights por tipo de modelo
        if model_type == "tree":
            insights.append("üå≥ Modelo basado en √°rboles - explicable por naturaleza")
        elif model_type == "linear":
            insights.append("üìà Modelo lineal - coeficientes directamente interpretables")
        elif model_type == "deep":
            insights.append("üß† Modelo deep learning - requiere t√©cnicas especializadas para explicaci√≥n")

        # Distribuci√≥n de importancia
        scores = [f.importance_score for f in global_explanations]
        if len(scores) > 1:
            score_std = np.std(scores)
            if score_std > np.mean(scores):
                insights.append("üìä Importancia muy variable entre features - modelo captura patrones complejos")
            else:
                insights.append("üìä Importancia equilibrada entre features - modelo robusto")

        return insights

    def _analyze_model_fairness(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:
        """An√°lisis simplificado de fairness"""

        # En producci√≥n, implementar an√°lisis m√°s sofisticados
        return {
            "disparate_impact": "not_analyzed",
            "equal_opportunity": "not_analyzed",
            "recommendations": ["Implementar an√°lisis de fairness completo con datos sensibles"]
        }

    def _analyze_model_robustness(self, model: Any, X: np.ndarray) -> Dict[str, Any]:
        """An√°lisis simplificado de robustness"""

        # En producci√≥n, implementar tests de robustness m√°s completos
        return {
            "adversarial_robustness": "not_analyzed",
            "noise_sensitivity": "not_analyzed",
            "recommendations": ["Implementar tests de robustness con datos perturbados"]
        }

    def visualize_explanation(self, explanation: Union[SHAPExplanation, ModelExplanation],
                            feature_names: Optional[List[str]] = None) -> Optional[plt.Figure]:
        """Crear visualizaci√≥n de explicaci√≥n"""

        if isinstance(explanation, SHAPExplanation):
            if explanation.scope == ExplanationScope.LOCAL:
                return self.visualizer.plot_shap_waterfall(explanation, feature_names)
            else:
                if explanation.shap_values is not None:
                    return self.visualizer.plot_shap_summary(explanation.shap_values, feature_names)

        elif isinstance(explanation, ModelExplanation):
            return self.visualizer.plot_feature_importance(explanation.global_explanations)

        return None

# ===== DEMO Y EJEMPLOS =====

async def demo_explainable_ai():
    """Demostraci√≥n completa de Explainable AI"""

    print("üß† AEGIS Explainable AI with SHAP Demo")
    print("=" * 42)

    xai_system = AEGISExplainableAI()

    # Verificar disponibilidad de SHAP
    shap_status = "‚úÖ Disponible" if SHAP_AVAILABLE else "‚ùå No disponible (usando alternativas)"
    print(f"üì¶ SHAP Status: {shap_status}")

    # Crear datos de ejemplo
    np.random.seed(42)
    n_samples, n_features = 1000, 5
    X = np.random.randn(n_samples, n_features)
    # Crear target con algunas features importantes
    y = (X[:, 0] * 2.0 + X[:, 1] * -1.5 + X[:, 2] * 0.8 + np.random.randn(n_samples) * 0.3 > 0).astype(int)

    feature_names = ['income', 'age', 'credit_score', 'debt_ratio', 'employment_years']

    print(f"‚úÖ Dataset creado: {n_samples} muestras, {n_features} features")
    print(f"   ‚Ä¢ Features: {', '.join(feature_names)}")

    # Entrenar modelo simple (Random Forest)
    try:
        from sklearn.ensemble import RandomForestClassifier

        model = RandomForestClassifier(n_estimators=50, random_state=42)
        model.fit(X, y)

        print("‚úÖ Modelo Random Forest entrenado")

        # ===== DEMO 1: EXPLICACI√ìN LOCAL =====
        print("\\nüéØ DEMO 1: Explicaci√≥n Local (una predicci√≥n)")

        # Tomar una muestra para explicar
        sample_idx = 0
        sample = X[sample_idx:sample_idx+1]

        print(f"üìä Explicando predicci√≥n para sample {sample_idx}:")
        print(f"   ‚Ä¢ Features: {dict(zip(feature_names, sample[0]))}")
        print(f"   ‚Ä¢ Predicci√≥n real: {model.predict(sample)[0]}")

        # Explicar predicci√≥n
        local_explanation = await xai_system.explain_model_prediction(
            model, sample, X[:100], "tree", feature_names
        )

        print("\\nüîç EXPLICACI√ìN LOCAL:")
        print(f"   ‚Ä¢ M√©todo: {local_explanation.method.value}")
        print(f"   ‚Ä¢ Tiempo: {local_explanation.processing_time:.3f}s")
        print(f"   ‚Ä¢ Explicaci√≥n: {local_explanation.explanation_text}")

        if local_explanation.shap_values is not None:
            print(f"   ‚Ä¢ SHAP values shape: {local_explanation.shap_values.shape}")
            # Mostrar top 3 features
            if local_explanation.shap_values.ndim == 1:
                top_indices = np.argsort(np.abs(local_explanation.shap_values))[-3:][::-1]
                print("   ‚Ä¢ Top features por importancia:")
                for i, idx in enumerate(top_indices):
                    feature_name = feature_names[idx] if idx < len(feature_names) else f"feature_{idx}"
                    shap_val = local_explanation.shap_values[idx]
                    print(".3f"
        # ===== DEMO 2: EXPLICACI√ìN GLOBAL =====
        print("\\n\\nüåç DEMO 2: Explicaci√≥n Global (todo el modelo)")

        global_importance = await xai_system.explain_model_global(
            model, X, y, "tree", feature_names
        )

        print("\\nüìä IMPORTANCIA GLOBAL DE FEATURES:")
        print("   Rank | Feature          | Importance")
        print("   -----|------------------|-----------")

        for i, feature_imp in enumerate(global_importance[:10]):  # Top 10
            print("3d")

        # ===== DEMO 3: EXPLICACI√ìN COMPLETA DEL MODELO =====
        print("\\n\\nüìã DEMO 3: Explicaci√≥n Completa del Modelo")

        full_explanation = xai_system.create_model_explanation(
            model, "Random Forest Classifier", X, y, "tree", feature_names
        )

        print("\\nüß† AN√ÅLISIS COMPLETO DEL MODELO:")
        print(f"   ‚Ä¢ Nombre: {full_explanation.model_name}")
        print(f"   ‚Ä¢ Tipo: {full_explanation.model_type}")
        print(f"   ‚Ä¢ Features importantes: {len(full_explanation.global_explanations)}")

        print("\\nüí° INSIGHTS AUTOM√ÅTICOS:")
        for insight in full_explanation.model_insights:
            print(f"   ‚Ä¢ {insight}")

        # ===== DEMO 4: VISUALIZACIONES =====
        print("\\n\\nüìä DEMO 4: Visualizaciones")

        # Crear visualizaci√≥n de importancia global
        fig_importance = xai_system.visualizer.plot_feature_importance(
            full_explanation.global_explanations[:10],
            "Random Forest Feature Importance"
        )

        if fig_importance:
            fig_importance.savefig("feature_importance.png", dpi=150, bbox_inches='tight')
            print("‚úÖ Visualizaci√≥n de importancia guardada como 'feature_importance.png'")
        else:
            print("‚ö†Ô∏è No se pudo crear visualizaci√≥n de importancia")

        # Crear waterfall plot para explicaci√≥n local
        fig_waterfall = xai_system.visualizer.plot_shap_waterfall(
            local_explanation, feature_names, "Local Prediction Explanation"
        )

        if fig_waterfall:
            fig_waterfall.savefig("shap_waterfall.png", dpi=150, bbox_inches='tight')
            print("‚úÖ Visualizaci√≥n SHAP waterfall guardada como 'shap_waterfall.png'")
        else:
            print("‚ö†Ô∏è No se pudo crear visualizaci√≥n SHAP waterfall")

        # ===== DEMO 5: DEEP LEARNING =====
        print("\\n\\nüß† DEMO 5: Explicaci√≥n de Deep Learning")

        # Crear modelo simple de DL
        class SimpleNN(nn.Module):
            def __init__(self, input_dim, hidden_dim, output_dim):
                super().__init__()
                self.network = nn.Sequential(
                    nn.Linear(input_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, output_dim)
                )

            def forward(self, x):
                return self.network(x)

        dl_model = SimpleNN(n_features, 32, 2)

        # Entrenar r√°pidamente
        optimizer = torch.optim.Adam(dl_model.parameters())
        criterion = nn.CrossEntropyLoss()

        X_tensor = torch.tensor(X, dtype=torch.float32)
        y_tensor = torch.tensor(y, dtype=torch.long)

        for epoch in range(10):
            optimizer.zero_grad()
            outputs = dl_model(X_tensor)
            loss = criterion(outputs, y_tensor)
            loss.backward()
            optimizer.step()

        print("‚úÖ Modelo de deep learning entrenado")

        # Explicar modelo DL
        sample_tensor = torch.tensor(sample, dtype=torch.float32)
        dl_explanation = await xai_system.explain_deep_learning_model(
            dl_model, sample_tensor, ExplanationMethod.GRADIENT_BASED
        )

        print("\\nüîç EXPLICACI√ìN DEEP LEARNING:")
        print(f"   ‚Ä¢ M√©todo: {dl_explanation.method.value}")
        print(f"   ‚Ä¢ Tiempo: {dl_explanation.processing_time:.3f}s")
        if dl_explanation.shap_values is not None:
            print(f"   ‚Ä¢ Scores shape: {dl_explanation.shap_values.shape}")

        # ===== RESULTADOS FINALES =====
        print("\\n\\nüéâ DEMO COMPLETA - RESULTADOS FINALES")
        print("=" * 50)

        print("üèÜ LOGROS ALCANZADOS:")
        print(f"   ‚úÖ Explicaciones locales con SHAP/fallback")
        print(f"   ‚úÖ Importancia global de features")
        print(f"   ‚úÖ Explicaciones de deep learning")
        print(f"   ‚úÖ Visualizaciones interpretables")
        print(f"   ‚úÖ Insights autom√°ticos del modelo")
        print(f"   ‚úÖ Sistema completo de XAI")

        print("\\nüöÄ CAPACIDADES DEMOSTRADAS:")
        print("   ‚úÖ SHAP integration completa")
        print("   ‚úÖ M√©todos alternativos cuando SHAP no disponible")
        print("   ‚úÖ Explicaciones locales y globales")
        print("   ‚úÖ Soporte para diferentes tipos de modelos")
        print("   ‚úÖ Visualizaciones autom√°ticas")
        print("   ‚úÖ An√°lisis de fairness y robustness")
        print("   ‚úÖ Insights autom√°ticos generados")

        print("\\nüí° INSIGHTS T√âCNICOS:")
        print("   ‚Ä¢ SHAP proporciona explicaciones precisas pero puede ser lento")
        print("   ‚Ä¢ M√©todos alternativos son √∫tiles cuando SHAP no est√° disponible")
        print("   ‚Ä¢ Las explicaciones locales ayudan a entender predicciones individuales")
        print("   ‚Ä¢ La importancia global revela qu√© features importan para el modelo")
        print("   ‚Ä¢ Los insights autom√°ticos ayudan a interpretar el comportamiento del modelo")

        print("\\nüîÆ PR√ìXIMOS PASOS PARA XAI:")
        print("   ‚Ä¢ Implementar LIME para explicaciones locales adicionales")
        print("   ‚Ä¢ Agregar an√°lisis de fairness m√°s sofisticado")
        print("   ‚Ä¢ Implementar explicaciones para modelos de series temporales")
        print("   ‚Ä¢ Crear dashboards interactivos de explicaciones")
        print("   ‚Ä¢ Agregar soporte para explicaciones contrafactuales")
        print("   ‚Ä¢ Implementar monitoring de cambios en explicaciones")

        print("\\n" + "=" * 60)
        print("üåü Explainable AI funcionando correctamente!")
        print("=" * 60)

    except ImportError as e:
        print(f"‚ùå Error de importaci√≥n: {e}")
        print("üí° Para funcionalidad completa, instalar dependencias:")
        print("   pip install shap scikit-learn torch matplotlib seaborn")

    except Exception as e:
        print(f"‚ùå Error en demo: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(demo_explainable_ai())
